{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2150841\n",
      "2150841\n",
      "call resolver void int step stop resolving debugger closed\n",
      "ssss debugger closed eeee\n",
      "(2150841, 168)\n",
      "(2150841, 6)\n",
      "1\n",
      "2\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0   670   677     2 27224   240  1458]\n",
      "clone flags string mips policy cpu\n",
      "cpu policy mips string flags clone\n",
      "[  1   3 717   2   0   0]\n",
      "ssss get flags eeee\n",
      "ssss get flags eeee\n",
      "(2150841, 5)\n",
      "(2150841, 5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "result_path = '/home1/michael/ICSE2020/ICSE2020_Code/result/'\n",
    "data_path = '/home1/michael/ICSE2020/ICSE2020_Code/data/'\n",
    "method_name_path = data_path + 'parsedMethodNameTokens.txt'\n",
    "context_path = data_path + 'parsedMethodContextTokens.txt'\n",
    "# method_name_path = data_path + 'filter_name_tokens.txt'\n",
    "# context_path = data_path + 'filter_context_tokens.txt'\n",
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "\n",
    "\n",
    "def load_data(src=True, start=\"\", end=\"\"):\n",
    "    results = []\n",
    "    if src:\n",
    "        path = context_path\n",
    "        with open(path) as file:\n",
    "            texts = [start + line.strip() + end for line in file]\n",
    "            results.extend(texts)\n",
    "    else:\n",
    "        path = method_name_path\n",
    "        with open(path) as file:\n",
    "            texts = [start + line.strip() + end for line in file]\n",
    "            results.extend(texts)\n",
    "    return results\n",
    "\n",
    "\n",
    "data_src = load_data(src=True)\n",
    "data_dest = load_data(src=False, start=mark_start, end=mark_end)\n",
    "\n",
    "print(len(data_src))\n",
    "print(len(data_dest))\n",
    "\n",
    "idx = 10000\n",
    "print(data_src[idx])\n",
    "print(data_dest[idx])\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, padding, reverse=False, num_words=None):\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words, filters='\"#$.?@\\\\^_`~\\t\\n', split=' ')\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n",
    "\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "\n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "tokenizer_src = TokenizerWrap(texts=data_src,\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=num_words)\n",
    "\n",
    "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)\n",
    "\n",
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)\n",
    "\n",
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "print(token_start)\n",
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "print(token_end)\n",
    "\n",
    "idx = 0\n",
    "print(tokens_src[idx])\n",
    "print(tokenizer_src.tokens_to_string(tokens_src[idx]))\n",
    "print(data_src[idx])\n",
    "\n",
    "print(tokens_dest[idx])\n",
    "print(tokenizer_dest.tokens_to_string(tokens_dest[idx]))\n",
    "print(data_dest[idx])\n",
    "\n",
    "encoder_input_data = tokens_src\n",
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "print(decoder_input_data.shape)\n",
    "\n",
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "print(decoder_output_data.shape)\n",
    "\n",
    "idx = 22\n",
    "decoder_input_data[idx]\n",
    "decoder_output_data[idx]\n",
    "tokenizer_dest.tokens_to_string(decoder_input_data[idx])\n",
    "tokenizer_dest.tokens_to_string(decoder_output_data[idx])\n",
    "\n",
    "encoder_input = Input(shape=(None,), name='encoder_input')\n",
    "\n",
    "# embedding_size = 128\n",
    "embedding_size = 512\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='encoder_embedding')\n",
    "\n",
    "state_size = 512\n",
    "\n",
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
    "                   return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
    "                   return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
    "                   return_sequences=False)\n",
    "\n",
    "\n",
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "\n",
    "    return encoder_output\n",
    "\n",
    "\n",
    "encoder_output = connect_encoder()\n",
    "\n",
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                              name='decoder_initial_state')\n",
    "\n",
    "decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "\n",
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='decoder_embedding')\n",
    "\n",
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
    "                   return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
    "                   return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
    "                   return_sequences=True)\n",
    "\n",
    "decoder_dense = Dense(num_words,\n",
    "                      activation='linear',\n",
    "                      name='decoder_output')\n",
    "\n",
    "\n",
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "\n",
    "    return decoder_output\n",
    "\n",
    "\n",
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])\n",
    "\n",
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])\n",
    "\n",
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])\n",
    "\n",
    "\n",
    "# model_train.compile(optimizer=optimizer,\n",
    "#                     loss='sparse_categorical_crossentropy')\n",
    "\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "\n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean\n",
    "\n",
    "\n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "\n",
    "model_train.compile(optimizer=optimizer,\n",
    "                    loss=sparse_cross_entropy)\n",
    "\n",
    "path_checkpoint = '14m_fse19_return_paras_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./14m_fse19_return_paras_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 512)    15360000    encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru1 (GRU)              (None, None, 512)    1575936     encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru2 (GRU)              (None, None, 512)    1575936     encoder_gru1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 512)    15360000    decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru3 (GRU)              (None, 512)          1575936     encoder_gru2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru1 (GRU)              (None, None, 512)    1575936     decoder_embedding[0][0]          \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru2 (GRU)              (None, None, 512)    1575936     decoder_gru1[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru3 (GRU)              (None, None, 512)    1575936     decoder_gru2[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 30000)  15390000    decoder_gru3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 55,565,616\n",
      "Trainable params: 55,565,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_train.load_weights(result_path + \"14m_fse19_v11.hdf5\") #TrainingDataAllTokens + NegativeItems\n",
    "# model_train.load_weights(result_path + \"14m_fse19_v10.hdf5\") #TrainingDataLT94Tokens\n",
    "# model_train.load_weights(result_path + \"14m_icse20_v2.hdf5\") #1 epoch\n",
    "# model_train.load_weights(result_path + \"14m_icse20_v3.hdf5\") #3 epoch\n",
    "# model_train.load_weights(result_path + \"14m_icse20_v4.hdf5\") #5 epoch\n",
    "# model_train.load_weights(result_path + \"icse20_filteredTestRepos_v2.hdf5\") #10 epoch\n",
    "# model_train.load_weights(result_path + \"1e-3_1024_50000.hdf5\") #10 epoch\n",
    "model_train.load_weights(result_path + \"1e-3_512_512_512.hdf5\") #10 epoch\n",
    "model_train.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transform eeee\n"
     ]
    }
   ],
   "source": [
    "def trans(input_text):\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        x_data = \\\n",
    "            {\n",
    "                'decoder_initial_state': initial_state,\n",
    "                'decoder_input': decoder_input_data\n",
    "            }\n",
    "\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "\n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "print(trans(input_text=data_src[idx]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432\n",
      "145.17074298858643\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# test_path = '/home1/likejun/BadNameDetectionICSE2020/test_noreal_more_46_0228/consistent/'\n",
    "# test_path = '/home1/likejun/BadNameDetectionICSE2020/test_noreal_more_46_0228/inconsistent/'\n",
    "# test_path = '/home1/likejun/BadNameDetectionICSE2020/test_real_more_46_0228/inconsistent/'\n",
    "# test_path = '/home1/michael/ICSE2020/ICSE2020_Code/icse2020MNR/'\n",
    "# test_path = '/home1/michael/ICSE2020/ICSE2020_Code/test_noreal_more_46/consistent/'\n",
    "# test_path = '/home1/michael/ICSE2020/ICSE2020_Code/test_noreal_more_46/inconsistent/'\n",
    "test_path = '/home1/michael/BadMethodName/Output_TestingRepos/InconsistentMethods/ICSE2020TestData/Tuning/'\n",
    "# test_path = '/home1/michael/BadMethodName/Output_TestingRepos/InconsistentMethods/ICSE2020TestData/RealRatio/'\n",
    "# test_path = '/home1/michael/BadMethodName/Output_TestingRepos/InconsistentMethods/ICSE2020TestData/Balanced/'\n",
    "# test_path = '/home1/michael/ICSE2020/ICSE2020_Code/test_real_more_46/inconsistent/'\n",
    "\n",
    "all_srcs_test = []\n",
    "\n",
    "path = test_path + 'parsedMethodContextTokens.txt'\n",
    "with open(path) as file:\n",
    "    texts = [line.strip() for line in file]\n",
    "    all_srcs_test.extend(texts)\n",
    "print(len(all_srcs_test))\n",
    "name_list = []\n",
    "start = time.time()\n",
    "with open(test_path + 'predictedNames.txt', 'w') as f:\n",
    "    for body in all_srcs_test:\n",
    "        name = trans(input_text=body)\n",
    "        name1 = name[1:]\n",
    "        f.write(name1.replace(\" eeee\", \"\") + '\\n')\n",
    "        name_list.append(1)\n",
    "        if len(name_list) % 1000 == 0:\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "            start = end"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}